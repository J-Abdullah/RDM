{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalAveragePooling1D,BatchNormalization, Flatten, Dense, Multiply,multiply, Reshape, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report,mean_squared_error,accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to extract MFCC features\n",
    "max_pad_len=862\n",
    "def extract_mfcc_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast', duration=20) \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        return mfccs\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\J.Abdullah\\\\Desktop\\\\Lea!n\\\\DataScience\\\\Respiratory_Sound_Database\\\\Respiratory_Sound_Database\\\\audio_and_txt_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Define the path to the audio files\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#mypath = \"/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m mypath\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mJ.Abdullah\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDesktop\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLea!n\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDataScience\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mRespiratory_Sound_Database\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mRespiratory_Sound_Database\u001b[39m\u001b[39m\\\u001b[39m\u001b[39maudio_and_txt_files\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m filenames \u001b[39m=\u001b[39m [file \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m listdir(mypath) \u001b[39mif\u001b[39;00m (isfile(join(mypath, file)) \u001b[39mand\u001b[39;00m file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.wav\u001b[39m\u001b[39m'\u001b[39m))]\n\u001b[0;32m      6\u001b[0m \u001b[39m# Create empty lists to store features and labels\u001b[39;00m\n\u001b[0;32m      7\u001b[0m features \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\J.Abdullah\\\\Desktop\\\\Lea!n\\\\DataScience\\\\Respiratory_Sound_Database\\\\Respiratory_Sound_Database\\\\audio_and_txt_files'"
     ]
    }
   ],
   "source": [
    "# Define the path to the audio files\n",
    "#mypath = \"/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/\"\n",
    "mypath=r\"E:\\Lea!n\\DataScience\\Respiratory_Sound_Database\\Respiratory_Sound_Database\\audio_and_txt_files\"\n",
    "filenames = [file for file in listdir(mypath) if (isfile(join(mypath, file)) and file.endswith('.wav'))]\n",
    "\n",
    "# Create empty lists to store features and labels\n",
    "features = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each sound file and extract the features\n",
    "for file_name in filenames:\n",
    "    # Extract MFCC features and append to the features list\n",
    "    mfccs = extract_mfcc_features(join(mypath, file_name))\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        \n",
    "        # Extract the label from the file name (assuming the label is in the file name format)\n",
    "        label = file_name.split('_')[2]  # Assuming the label is the third element in the file name\n",
    "        labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features:  (920, 40, 862)\n",
      "Shape of labels:  (920,)\n"
     ]
    }
   ],
   "source": [
    "# Convert the labels into numerical format using Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Convert the lists into numpy arrays\n",
    "features = np.array(features)\n",
    "labels_encoded = np.array(labels_encoded)\n",
    "\n",
    "# Print the shape of the feature matrix and the label array\n",
    "print(\"Shape of features: \", features.shape)\n",
    "print(\"Shape of labels: \", labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 862)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nChanges made to fix the issue:\\n\\nReplace Multiply() with multiply() from tensorflow.keras.layers.\\nUse Dense(x.shape[1] * x.shape[2], activation='softmax') to match the shape of the intermediate tensor for attention weights calculation.\\nReshape the attention weights using Reshape((x.shape[1], x.shape[2], 1)) to obtain the same shape as the intermediate tensor.\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the Attention CNN model\n",
    "def build_attention_cnn_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Attention layer\n",
    "    attention_weights = Dense(1, activation='tanh')(x)\n",
    "    attention_weights = Flatten()(attention_weights)\n",
    "    attention_weights = Dense(x.shape[1] * x.shape[2], activation='softmax')(attention_weights)\n",
    "    attention_weights = Reshape((x.shape[1], x.shape[2], 1))(attention_weights)\n",
    "    x = multiply([x, attention_weights])\n",
    "\n",
    "    # Global Average Pooling and Dense layers\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "'''\n",
    "Changes made to fix the issue:\n",
    "\n",
    "Replace Multiply() with multiply() from tensorflow.keras.layers.\n",
    "Use Dense(x.shape[1] * x.shape[2], activation='softmax') to match the shape of the intermediate tensor for attention weights calculation.\n",
    "Reshape the attention weights using Reshape((x.shape[1], x.shape[2], 1)) to obtain the same shape as the intermediate tensor.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape for both models\n",
    "max_pad_len = max(mfcc.shape[1] for mfcc in features)\n",
    "attention_cnn_input_shape = (40, max_pad_len, 1)  # MFCC features shape\n",
    "num_classes = len(np.unique(labels_encoded))\n",
    "\n",
    "# Build the Attention CNN model\n",
    "attention_cnn_model = build_attention_cnn_model(attention_cnn_input_shape, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_cnn_model.ouput.shape= (None, 7)\n",
      "attention_cnn_model.input.shape= (None, 40, 862, 1)\n",
      "features.shape= (920, 40, 862)\n",
      "labels_encoded.shape= (920,)\n"
     ]
    }
   ],
   "source": [
    "print('attention_cnn_model.ouput.shape=',attention_cnn_model.output.shape)\n",
    "print('attention_cnn_model.input.shape=',attention_cnn_model.input.shape)\n",
    "print('features.shape=',features.shape)\n",
    "print('labels_encoded.shape=',labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 40, 862, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 38, 860, 32)  320         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 19, 430, 32)  0          ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 17, 428, 64)  18496       ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 8, 214, 64)  0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 8, 214, 1)    65          ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 1712)         0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1712)         2932656     ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 8, 214, 1)    0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 8, 214, 64)   0           ['max_pooling2d_7[0][0]',        \n",
      "                                                                  'reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 64)          0           ['multiply_3[0][0]']             \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 128)          8320        ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 7)            903         ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,960,760\n",
      "Trainable params: 2,960,760\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the hybrid model\n",
    "attention_cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "attention_cnn_model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_encoded, test_size=0.3, random_state=42,shuffle=True)\n",
    "# Convert labels to one-hot encoded format\n",
    "y_train_one_hot = to_categorical(y_train, num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# Define callbacks for model training\n",
    "checkpoint = ModelCheckpoint('attention_cnn_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.7312 - accuracy: 0.3178 - val_loss: 1.7225 - val_accuracy: 0.3385\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.7190 - accuracy: 0.3195 - val_loss: 1.7396 - val_accuracy: 0.3538\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.7106 - accuracy: 0.3264 - val_loss: 1.7446 - val_accuracy: 0.3692\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 22s 1s/step - loss: 1.7080 - accuracy: 0.3109 - val_loss: 1.6927 - val_accuracy: 0.3846\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 24s 1s/step - loss: 1.7044 - accuracy: 0.3402 - val_loss: 1.6990 - val_accuracy: 0.3846\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 25s 1s/step - loss: 1.6998 - accuracy: 0.3437 - val_loss: 1.7285 - val_accuracy: 0.4154\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 25s 1s/step - loss: 1.7040 - accuracy: 0.3299 - val_loss: 1.7305 - val_accuracy: 0.3846\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 23s 1s/step - loss: 1.6952 - accuracy: 0.3195 - val_loss: 1.7166 - val_accuracy: 0.3385\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 26s 1s/step - loss: 1.6953 - accuracy: 0.3333 - val_loss: 1.7718 - val_accuracy: 0.3692\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 26s 1s/step - loss: 1.6895 - accuracy: 0.3299 - val_loss: 1.7109 - val_accuracy: 0.3692\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 23s 1s/step - loss: 1.6874 - accuracy: 0.3299 - val_loss: 1.6680 - val_accuracy: 0.3231\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 23s 1s/step - loss: 1.6925 - accuracy: 0.3264 - val_loss: 1.6852 - val_accuracy: 0.3846\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 23s 1s/step - loss: 1.6806 - accuracy: 0.3316 - val_loss: 1.7198 - val_accuracy: 0.3231\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 28s 1s/step - loss: 1.6682 - accuracy: 0.3230 - val_loss: 1.6883 - val_accuracy: 0.4000\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 25s 1s/step - loss: 1.6695 - accuracy: 0.3316 - val_loss: 1.6832 - val_accuracy: 0.4000\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 34s 2s/step - loss: 1.6658 - accuracy: 0.3472 - val_loss: 1.6881 - val_accuracy: 0.4000\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 34s 2s/step - loss: 1.6892 - accuracy: 0.3143 - val_loss: 1.6973 - val_accuracy: 0.3385\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 31s 2s/step - loss: 1.6543 - accuracy: 0.3627 - val_loss: 1.7051 - val_accuracy: 0.3846\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 31s 2s/step - loss: 1.6490 - accuracy: 0.3472 - val_loss: 1.7876 - val_accuracy: 0.3385\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 31s 2s/step - loss: 1.6644 - accuracy: 0.3472 - val_loss: 1.7113 - val_accuracy: 0.3385\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.6424 - accuracy: 0.3575 - val_loss: 1.7170 - val_accuracy: 0.2462\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 22s 1s/step - loss: 1.6417 - accuracy: 0.3713 - val_loss: 1.6931 - val_accuracy: 0.4154\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 23s 1s/step - loss: 1.6416 - accuracy: 0.3610 - val_loss: 1.7212 - val_accuracy: 0.3077\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 26s 1s/step - loss: 1.6368 - accuracy: 0.3679 - val_loss: 1.6823 - val_accuracy: 0.3846\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 23s 1s/step - loss: 1.6279 - accuracy: 0.3523 - val_loss: 1.7579 - val_accuracy: 0.2615\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 23s 1s/step - loss: 1.6253 - accuracy: 0.3575 - val_loss: 1.7180 - val_accuracy: 0.3231\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 24s 1s/step - loss: 1.6234 - accuracy: 0.3696 - val_loss: 1.7202 - val_accuracy: 0.4000\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 23s 1s/step - loss: 1.6186 - accuracy: 0.3834 - val_loss: 1.7073 - val_accuracy: 0.3692\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 24s 1s/step - loss: 1.6245 - accuracy: 0.3489 - val_loss: 1.7331 - val_accuracy: 0.2923\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 23s 1s/step - loss: 1.6201 - accuracy: 0.3644 - val_loss: 1.6678 - val_accuracy: 0.4154\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 24s 1s/step - loss: 1.6305 - accuracy: 0.3800 - val_loss: 1.7856 - val_accuracy: 0.2923\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 28s 1s/step - loss: 1.6164 - accuracy: 0.3938 - val_loss: 1.7580 - val_accuracy: 0.3077\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 25s 1s/step - loss: 1.6148 - accuracy: 0.3748 - val_loss: 1.6950 - val_accuracy: 0.3231\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 28s 1s/step - loss: 1.5982 - accuracy: 0.3782 - val_loss: 1.6776 - val_accuracy: 0.3692\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 1.5959 - accuracy: 0.3661 - val_loss: 1.7379 - val_accuracy: 0.2462\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.6007 - accuracy: 0.3713 - val_loss: 1.7145 - val_accuracy: 0.2462\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 27s 1s/step - loss: 1.6083 - accuracy: 0.3661 - val_loss: 1.7854 - val_accuracy: 0.2615\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 26s 1s/step - loss: 1.5942 - accuracy: 0.3851 - val_loss: 1.7456 - val_accuracy: 0.2923\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 27s 1s/step - loss: 1.6084 - accuracy: 0.3748 - val_loss: 1.7044 - val_accuracy: 0.2769\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 22s 1s/step - loss: 1.5847 - accuracy: 0.4007 - val_loss: 1.6980 - val_accuracy: 0.3077\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 25s 1s/step - loss: 1.5808 - accuracy: 0.3851 - val_loss: 1.6912 - val_accuracy: 0.3385\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 22s 1s/step - loss: 1.5953 - accuracy: 0.3886 - val_loss: 1.7073 - val_accuracy: 0.2462\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.5826 - accuracy: 0.3886 - val_loss: 1.7036 - val_accuracy: 0.2923\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 23s 1s/step - loss: 1.5740 - accuracy: 0.3972 - val_loss: 1.7207 - val_accuracy: 0.2615\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.5691 - accuracy: 0.3990 - val_loss: 1.7124 - val_accuracy: 0.2923\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 22s 1s/step - loss: 1.5759 - accuracy: 0.3851 - val_loss: 1.7012 - val_accuracy: 0.3538\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 19s 1s/step - loss: 1.5777 - accuracy: 0.3765 - val_loss: 1.7615 - val_accuracy: 0.2615\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.5689 - accuracy: 0.3972 - val_loss: 1.6989 - val_accuracy: 0.2923\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 20s 1s/step - loss: 1.5679 - accuracy: 0.3886 - val_loss: 1.7225 - val_accuracy: 0.2462\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.5612 - accuracy: 0.4059 - val_loss: 1.7134 - val_accuracy: 0.3077\n"
     ]
    }
   ],
   "source": [
    "history = attention_cnn_model.fit(\n",
    "    X_train, y_train_one_hot,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 2s 250ms/step\n",
      "[0.10067964 0.14811072 0.08624099 0.06991067 0.4086174  0.1816766\n",
      " 0.00476404]\n",
      "4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.30      0.29        47\n",
      "           1       0.29      0.48      0.36        48\n",
      "           2       0.00      0.00      0.00        21\n",
      "           3       0.29      0.13      0.18        39\n",
      "           4       0.31      0.38      0.34        39\n",
      "           5       0.31      0.28      0.29        40\n",
      "           6       0.56      0.64      0.60        42\n",
      "\n",
      "    accuracy                           0.34       276\n",
      "   macro avg       0.29      0.32      0.30       276\n",
      "weighted avg       0.32      0.34      0.32       276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\J.Abdullah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\J.Abdullah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\J.Abdullah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = attention_cnn_model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "print(y_pred[0])\n",
    "print(y_pred_labels[0])\n",
    "print(classification_report(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = attention_cnn_model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.3583032465945832\n",
      "Accuracy: 0.3442028985507246\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = np.sqrt(mean_squared_error(np.argmax(y_test_one_hot, axis=1), y_pred_labels))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(np.argmax(y_test_one_hot, axis=1), y_pred_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
